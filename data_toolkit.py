# -*- coding: utf-8 -*-
"""Data Toolkit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LSmEyjTdWiIttdbfgWrFFWVz3R-jfZLY

#Data Toolkit

#Theory Questions

Q.1 What is NumPy, and why is it widely used in Python?

     --> NumPy, short for Numerical Python, is a fundamental open-source library in Python for scientific computing. Its core feature is the ndarray object, which is a powerful, homogeneous, multidimensional array. This array object allows for efficient storage and manipulation of large datasets. NumPy also provides a vast collection of high-level mathematical functions to operate on these arrays, including linear algebra, Fourier transforms, random number generation, and statistical operations.

Q.2 How does broadcasting work in NumPy?

     --> Broadcasting in NumPy is a mechanism that allows arithmetic operations to be performed on arrays of different shapes and sizes. It enables element-wise operations without requiring explicit loops or making unnecessary copies of data, which enhances performance and memory efficiency.

     How Broadcasting Works:

     Dimension Alignment (Right to Left):
       - NumPy compares the dimensions of the two arrays involved in the operation, starting from the trailing (rightmost) dimension and moving towards the leading (leftmost) dimension.

     Compatibility Rules:
       - For two dimensions to be compatible, they must meet one of the following criteria:
       * They are equal in size.
       * One of them has a size of 1.

     Broadcasting Action:
       * If dimensions are equal, no action is needed.
       * If one dimension has a size of 1, NumPy "stretches" or "broadcasts" that dimension to match the size of the other array's corresponding dimension. This is done conceptually without physically replicating data, saving memory.
       * If dimensions are not compatible (i.e., unequal and neither is 1), a ValueError is raised, indicating that the arrays cannot be broadcast together.
      
     Implicit Dimension Expansion:
      - If one array has fewer dimensions than the other, NumPy implicitly adds leading dimensions of size 1 to the smaller array to match the number of dimensions of the larger array before applying the compatibility rules.

Q.3 What is a Pandas DataFrame?

     --> A Pandas DataFrame is a two-dimensional, labeled data structure in the Pandas library for Python. It is designed to handle tabular data, similar to a spreadsheet, a SQL table, or a dictionary of Series objects.

Q.4 Explain the use of the groupby() method in Pandas.

     --> The groupby() method in Pandas is a fundamental function used for analyzing and manipulating data based on groups within a DataFrame. It follows a "split-apply-combine" strategy:

     Split:
          - The DataFrame is divided into groups based on the values in one or more specified columns. Each unique combination of values in the chosen columns forms a distinct group.

     Apply:
         - A function or operation is applied independently to each of these individual groups. This can include:

         * Aggregation: Calculating a summary statistic for each group (e.g., sum, mean, count, min, max, standard deviation).
         * Transformation: Performing a group-specific computation that returns a Series or DataFrame with the same index as the original group (e.g., standardizing data within each group, filling missing values based on group statistics).
         * Filtration: Discarding entire groups or rows within groups based on a group-wise condition (e.g., keeping only groups with a certain number of members, filtering out data based on a group's mean).

     Combine:
         - The results from applying the function to each group are combined into a new DataFrame or Series, depending on the operation performed.

     Example Usage:

                   import pandas as pd

         data = {'Category': ['A', 'B', 'A', 'B', 'A', 'C'],
        'Value': [10, 15, 12, 18, 11, 20]}
         df = pd.DataFrame(data)

        # Group by 'Category' and calculate the mean of 'Value' for each category
        grouped_mean = df.groupby('Category')['Value'].mean()
        print("Grouped Mean:\n", grouped_mean)

        # Group by 'Category' and apply a transformation (e.g., z-score within each group)
        # from scipy.stats import zscore
        # transformed_data = df.groupby('Category')['Value'].transform(zscore)
        # print("\nTransformed Data (Z-score within groups):\n", transformed_data)


      - The groupby() method is highly versatile and essential for tasks like calculating statistics for different segments of data, performing group-wise data cleaning, and preparing data for further analysis or visualization.

Q.5 Why is Seaborn preferred for statistical visualizations?

     --> Seaborn is often preferred for statistical visualizations in Python due to several key advantages:

     Higher-Level Interface for Statistical Graphics:
        -Seaborn is built on top of Matplotlib but offers a higher-level interface specifically designed for statistical plotting. This simplifies the creation of complex statistical visualizations like heatmaps, violin plots, and pair plots, requiring less code compared to Matplotlib for similar results.

     Aesthetically Pleasing Defaults:
        - Seaborn provides attractive default styles and color palettes, making visualizations visually appealing with minimal effort. This allows users to focus on data interpretation rather than extensive design adjustments.

     Integrated Statistical Functions:
        - Seaborn seamlessly integrates statistical estimation and aggregation functions directly into its plotting functions. This enables users to visualize statistical relationships, distributions, and comparisons with built-in functionalities, such as plotting confidence intervals on bar plots or showing kernel density estimates.

     Seamless Pandas DataFrame Integration:
        - Seaborn is designed to work exceptionally well with Pandas DataFrames. It can directly use DataFrame column names for plotting, handle categorical variables automatically, and streamline the process of visualizing data directly from data structures, which is highly efficient for exploratory data analysis.

     Specialized Plot Types for Statistical Analysis:
        - Seaborn offers a range of specialized plot types tailored for statistical analysis, including:
        * Distribution plots: such as histplot, kdeplot, and displot for visualizing the distribution of a single variable.
        * Relational plots: like scatterplot and lineplot for showing relationships between variables.
        * Categorical plots: including boxplot, violinplot, swarmplot, and barplot for visualizing relationships between a categorical and a numerical variable.

     Focus on Data Understanding:
        - Seaborn's design aims to make visualization a central part of exploring and understanding data, providing dataset-oriented APIs that facilitate switching between different visual representations for a better understanding of the dataset.

Q.6 What are the differences between NumPy arrays and Python lists?

     --> The primary differences between NumPy arrays and Python lists revolve around performance, memory usage, data type handling, and functionality:

     Performance:
      - NumPy arrays are significantly faster for numerical operations, especially with large datasets, due to their underlying implementation in C and Fortran. Python lists are generally slower for mathematical computations as they require explicit looping for element-wise operations.

     Memory Efficiency:
      - NumPy arrays are more memory-efficient than Python lists. They store elements of a single data type in contiguous memory blocks, which allows for optimized memory access and reduced overhead. Python lists, being able to store heterogeneous data, allocate memory for each individual element, leading to higher memory consumption.

     Data Type Homogeneity:
      - NumPy arrays enforce homogeneity, meaning all elements within an array must be of the same data type (e.g., all integers or all floats). Python lists, conversely, are heterogeneous and can store elements of different data types (e.g., integers, strings, and booleans) within the same list.

     Functionality and Operations:
      - NumPy arrays offer a rich set of optimized mathematical functions and operations, including vectorized operations, broadcasting, and linear algebra routines, which are highly efficient for numerical computations. Python lists require manual iteration and implementation for similar operations.

     Flexibility:
      - Python lists are more flexible in terms of data modification and dynamic resizing. NumPy arrays are typically fixed in size once created, and resizing them can be less efficient than with lists.

     In summary, Python lists are suitable for general-purpose tasks requiring flexibility and handling of diverse data types, while NumPy arrays are specifically designed and optimized for efficient numerical computations and large-scale data manipulation, particularly in scientific computing and data science applications.

Q.7 What is a heatmap, and when should it be used?

     --> A heatmap is a data visualization that uses a grid of colored cells to represent the magnitude of values in a dataset, with color intensity corresponding to value. They are used to simplify complex data, reveal patterns, and highlight relationships or areas of high density or interest. Heatmaps are used across many fields, from website analytics to population studies, to understand user behavior, identify high-traffic locations, and optimize design or resource allocation.

Q.8 What does the term “vectorized operation” mean in NumPy?

     --> In NumPy, a vectorized operation refers to performing an operation on an entire array (or multiple arrays) at once, without explicitly writing a for loop to iterate through individual elements. This approach leverages NumPy's underlying C implementations, which are significantly faster and more efficient than traditional Python loops, especially when dealing with large datasets.

Q.9 How does Matplotlib differ from Plotly?

     --> Matplotlib and Plotly are both powerful Python libraries for data visualization, but they differ significantly in their primary focus and capabilities:

     Interactivity vs. Static Plots:
    
     Matplotlib: Primarily designed for creating static, publication-quality plots. While it offers some interactive features, its core strength lies in generating highly customizable, non-interactive visualizations suitable for reports and papers.

     Plotly: Excels in creating interactive, web-based visualizations. It provides built-in features like zooming, panning, hovering, and the ability to toggle data sets, making it ideal for exploratory data analysis, dashboards, and web applications.

     Customization and Control:

     Matplotlib: Offers extensive control over every aspect of a plot, allowing for fine-grained customization. This level of control can be complex and require more coding effort for elaborate designs.

     Plotly: Provides a more streamlined approach to creating aesthetically pleasing plots with less code, particularly with its Plotly Express API. While still offering customization, it prioritizes ease of use for common interactive chart types.

     Use Cases:

     Matplotlib: Best suited for generating static figures for academic papers, presentations, or situations where high-level customization and precise control over visual elements are paramount.

     Plotly: Ideal for creating dynamic and engaging visualizations for web applications, interactive dashboards, and scenarios where data exploration and user interaction are key.

Q.10 What is the significance of hierarchical indexing in Pandas?

      --> Hierarchical indexing, also known as MultiIndexing, is a significant feature in pandas for several reasons:

      Representing Higher-Dimensional Data:
        - It allows the representation of data with more than two dimensions within the familiar one-dimensional Series and two-dimensional DataFrame structures. This is achieved by creating multiple levels of indexing on one or both axes (rows and/or columns).

      Structured Data Organization:
        - MultiIndexes enable the creation of structured group relationships within your data. This is particularly useful for complex datasets with multiple levels of categorization, such as sales data grouped by region, then by product, and then by month.

      Efficient Data Selection and Subsetting:
        - Hierarchical indexes facilitate powerful and efficient data selection and subsetting. You can easily select data based on specific levels of the index or a combination of levels, making it simpler to access relevant subsets of your data.

      Simplified Grouping and Aggregation:
        - While groupby is a powerful tool, hierarchical indexing can often pre-organize data in a way that simplifies subsequent grouping and aggregation operations. The inherent structure of the MultiIndex can streamline analytical workflows.

      Enhanced Data Readability and Analysis:
        - By providing a clear and organized structure, hierarchical indexing improves the readability of your data and makes it easier to understand the relationships between different data points. This aids in more sophisticated data analysis and manipulation.

Q.11 What is the role of Seaborn’s pairplot() function?

      --> Seaborn's pairplot() function is a powerful tool for visualizing relationships between multiple variables in a dataset. Its primary role is to create a grid of plots, allowing for a quick and comprehensive overview of the data's structure and potential correlations.

Q.12 What is the purpose of the describe() function in Pandas?

      --> The describe() function in Pandas is used to generate descriptive statistics of a DataFrame or Series. Its primary purpose is to provide a quick and insightful summary of the data, aiding in initial data exploration and understanding.

Q.13 Why is handling missing data important in Pandas?

      --> Handling missing data in Pandas is crucial for ensuring the accuracy, reliability, and effectiveness of data analysis and machine learning models. The importance stems from several key reasons:

      Impact on Analysis Accuracy:
       - Missing values, often represented as NaN (Not a Number), can skew statistical calculations and lead to incorrect insights and conclusions. For example, calculating the mean of a column with missing values without proper handling can result in an inaccurate average.

      Model Performance:
       - Many machine learning algorithms cannot directly process datasets containing missing values. Ignoring them can lead to errors during model training or result in biased models that perform poorly on new data. Even algorithms that can handle missing values might still experience reduced performance if the missing data is not addressed.

      Bias Prevention:
       - Missing data can introduce systematic bias into a dataset if the missingness is not random. For instance, if data is missing more frequently for a specific demographic group, analysis based on the incomplete data will not accurately represent the entire population, leading to flawed decision-making.

      Data Integrity and Usability:
       - A clean dataset without missing values is more structured, complete, and easier to work with. This improves the overall quality of the data and makes it more readily usable for various analytical tasks and downstream applications.

      Statistical Power:
       - Addressing missing data, either through removal or imputation, can help maintain or even increase the statistical power of an analysis by allowing the use of more data points and a wider range of statistical techniques.

Q.14 What are the benefits of using Plotly for data visualization?

      --> Plotly offers several benefits for data visualization, making it a popular choice for creating interactive and insightful charts:

      Interactivity:
       - Plotly's core strength lies in its interactive features. Users can zoom, pan, hover over data points for details, and manipulate views, enhancing data exploration and analysis. This is particularly valuable for dashboards and web-based applications.

      Wide Range of Chart Types:
       - Plotly supports a comprehensive array of chart types, from basic line and bar charts to complex 3D plots, statistical visualizations, and specialized charts for finance, science, and geography. This versatility allows for diverse data representation.

      Ease of Use:
       - Plotly, especially with libraries like Plotly Express, provides a relatively simple and intuitive syntax, enabling users to create sophisticated visualizations with minimal code, even for those new to data visualization.

      Web-Based and Offline Capabilities:
       - Plotly visualizations are designed for web integration and can be easily embedded in web applications, shared as standalone HTML files, or used offline within programming environments like Python, R, or Julia.

      Customization Options:
       - Plotly offers extensive customization capabilities, allowing users to control virtually every aspect of a plot, including colors, fonts, labels, titles, annotations, and interactive elements, ensuring visualizations align with specific needs and branding.

      Integration with Data Ecosystems:
       - Plotly seamlessly integrates with popular data analysis libraries and tools, such as Pandas and NumPy in Python, facilitating data preparation and transformation before visualization.

      Real-Time Data Streaming:
       - Plotly supports real-time data streaming, enabling dynamic updates of visualizations as new data becomes available, which is crucial for monitoring and live dashboards.

      Cross-Platform Compatibility:
        - Plotly visualizations can be viewed and interacted with across various platforms and devices, including desktops, laptops, tablets, and smartphones, ensuring broad accessibility.

      Open-Source and Community Support:
       - As an open-source library, Plotly benefits from a large and active community, providing ample resources, documentation, and support for users.

Q.15 How does NumPy handle multidimensional arrays?

      --> NumPy handles multidimensional arrays through its core object, the ndarray (N-dimensional array). This object represents a homogeneous multidimensional array, meaning all elements within the array must be of the same data type.

      Key aspects of how NumPy handles multidimensional arrays include:

      Homogeneous Data Type:
       - All elements within a ndarray must share the same data type (e.g., all integers, all floats). This homogeneity allows for efficient storage and computation.

      Shape and Axes:
       - Multidimensional arrays are defined by their shape, which is a tuple of non-negative integers representing the size of each dimension (also called "axes"). For example, a 2D array (matrix) might have a shape of (rows, columns).

      Memory Layout:
       - While conceptually multidimensional, NumPy arrays are often stored contiguously in memory as a single, flattened 1D array. The shape and strides (the number of bytes to skip in memory to get to the next element along each dimension) attributes determine how elements are accessed and interpreted as a multidimensional structure.

      Indexing and Slicing:
       - NumPy provides powerful and flexible indexing and slicing mechanisms to access specific elements or subarrays within multidimensional arrays. This is achieved by providing indices for each dimension, or using slice objects (e.g., start:stop:step).

      Broadcasting:
       - NumPy employs a feature called "broadcasting," which allows operations between arrays of different shapes under certain compatibility rules. This eliminates the need for explicit loops and enables efficient element-wise operations on arrays with varying dimensions.

      Views vs. Copies:
       - Many NumPy operations on multidimensional arrays, particularly slicing and reshaping, can return "views" of the original data rather than creating new copies. This optimizes memory usage and performance by allowing multiple array objects to share the same underlying data buffer.

      Array Creation Functions:
       - NumPy offers various functions for creating multidimensional arrays, such as np.array() (from Python lists or other array-like objects), np.zeros(), np.ones(), np.empty(), and np.arange(), which can be specified with desired shapes.

Q.16 What is the role of Bokeh in data visualization?

      --> Bokeh is a Python library whose primary role in data visualization is to enable the creation of interactive, web-ready visualizations for modern web browsers. It focuses on providing a way to build sophisticated graphics and dashboards without requiring users to write JavaScript.

Q.17 A Explain the difference between apply() and map() in Pandas?

      --> In Pandas, both apply() and map() are used to apply functions or transformations to data, but they differ in their scope and primary use cases:

      1. map():

        * Scope: Exclusively for Pandas Series.
        * Purpose: Primarily used for element-wise substitution or mapping values in a Series based on a dictionary, another Series, or a function. It's ideal for replacing values or creating new values based on a lookup.
        * Input: Can accept a dictionary, a Series, or a callable (function).
        * Behavior: Substitutes each value in the Series with the corresponding mapped value. Missing values in the Series will result in NaN if no mapping is found, or the original value if a callable is used and it doesn't handle the value.

     Example:
                    import pandas as pd
                    s = pd.Series([1, 2, 3, 4])
                    mapping_dict = {1: 'one', 2: 'two'}
                    s_mapped = s.map(mapping_dict)
                    # s_mapped will be:
                    # 0    one
                    # 1    two
                    # 2    NaN
                    # 3    NaN
                    # dtype: object

      2. apply():

        * Scope: Can be used on both Pandas Series and DataFrames.
        * Purpose: More versatile, used to apply a function along an axis of a DataFrame (row-wise or column-wise) or to values in a Series. It's suitable for more complex operations, aggregations, or when the function requires multiple columns as input.
        * Input: Accepts a callable (function).
        * Behavior: When used on a DataFrame, it applies the function to each row (axis=1) or each column (axis=0). When used on a Series, it applies the function element-wise.

       Example:
                        import pandas as pd
                        df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})

                        # Apply a function to each column (axis=0)
                        df_applied_col = df.apply(lambda x: x.sum(), axis=0)
                        # df_applied_col will be:
                        # A     6
                        # B    15
                        # dtype: int64

                        # Apply a function element-wise on a Series
                        s = pd.Series([1, 2, 3])
                        s_applied = s.apply(lambda x: x * 2)
                        # s_applied will be:
                        # 0    2
                        # 1    4
                        # 2    6
                        # dtype: int64

        In summary:

        * map(): is specialized for element-wise value substitution in Series.
        * apply(): is more general-purpose, allowing function application along axes of DataFrames or element-wise on Series, and is better suited for complex operations.

Q.18 What are some advanced features of NumPy?

      --> Some advanced features of NumPy include:

      Universal Functions (ufuncs): These are vectorized wrappers around C functions that operate on every element of an array without using explicit Python loops. Ufuncs significantly improve performance for element-wise operations and support broadcasting, type casting, and additional methods like reduce and accumulate.

      Advanced Indexing: Beyond simple slicing, NumPy offers two powerful types of advanced indexing:
          
          * Integer Array Indexing (Fancy Indexing): Uses integer arrays to select multiple non-contiguous elements or rows and columns, creating a copy of the data rather than a view.

          * Boolean Array Indexing (Masking): Uses a boolean mask (an array of True and False values) to filter and select elements that meet a certain condition.

     Memory Mapping (memmap): This feature allows you to work with arrays larger than your available RAM by mapping the array from a file on disk. The data is read and written in chunks as needed, which is ideal for out-of-core computation on very large datasets.

     Structured Arrays: While standard NumPy arrays are homogeneous (contain a single data type), structured arrays allow you to create heterogeneous arrays with fields of different data types. This is useful for working with tabular data that mimics a database record or C struct.

     Stride Tricks: This technique allows for advanced, memory-efficient array manipulation by modifying the array's strides (the byte step between elements) without copying data. Functions like sliding_window_view() leverage this to create a "view" of overlapping elements for tasks like signal processing and image manipulation.

     Linear Algebra (linalg): The numpy.linalg module offers a wide range of functions for advanced linear algebra operations. These include:

         * Singular Value Decomposition (SVD): A powerful factorization technique used for data compression and dimensionality reduction.

         * Eigenvalue and eigenvector calculations.

         * Solving linear systems of equations.

     Fourier Transforms (fft): The numpy.fft module provides functions for performing Fourier transforms, which are useful for analyzing periodic signals and solving differential equations.

     Memory Management and Layout: NumPy provides control over memory management through functions like np.empty() and can optimize memory layout using C-style (row-major) or Fortran-style (column-major) ordering. This can improve performance for memory-intensive operations.

Q.19  How does Pandas simplify time series analysis?

       --> Pandas simplifies time series analysis through its specialized data structures and intuitive, powerful functions that are built for working with dates and times. The library was originally developed for financial modeling, giving it a robust set of tools for time-indexed data.

       Core components for time series

       Pandas represents time-related data with specific, powerful objects:

          * Timestamp: Represents a single, specific moment in time.

          * DatetimeIndex: A core data structure that uses Timestamp objects for a DataFrame or Series index. Setting the date/time data as the index allows for powerful and intuitive time-based operations.

          * Timedelta: Represents an absolute time duration (e.g., 5 minutes or 1 day).

          * Period: Represents a time span, such as a month or a quarter.

Q.20 What is the role of a pivot table in Pandas?

      --> In Pandas, a pivot table is a powerful tool used to summarize and aggregate data from a DataFrame in a way that resembles a pivot table in a spreadsheet program like Microsoft Excel. It allows you to transform "long" or unstructured data into a more organized, cross-tabulated format, which makes it easier to analyze, report, and gain new insights.

Q.21 A Why is NumPy’s array slicing faster than Python’s list slicing?

      --> NumPy's array slicing is faster than Python's list slicing primarily because NumPy slices often return a view of the original data, while Python list slices always create and return a new copy of the data. This difference in behavior is due to fundamental differences in how the two data structures are stored in memory.

Q.22 What are some common use cases for Seaborn?

      --> Seaborn is a Python data visualization library built on top of Matplotlib, designed to simplify the creation of attractive and informative statistical graphics. Common use cases for Seaborn include:

      Exploratory Data Analysis (EDA):
       - Seaborn excels at visualizing distributions of single variables (histograms, kernel density estimates), relationships between two variables (scatter plots, line plots), and relationships between multiple variables (pair plots, heatmaps). This helps in understanding data patterns, outliers, and potential correlations.

      Statistical Data Visualization:
       - It provides high-level functions for plotting statistical relationships, such as regression plots to visualize linear relationships, and categorical plots (box plots, violin plots, bar plots) to compare distributions or aggregates across different categories.

      Presenting Insights:
       - Seaborn's aesthetically pleasing default styles and color palettes make it suitable for creating publication-ready visualizations. It can be used to effectively communicate findings from data analysis to a wider audience.

      Working with Pandas DataFrames:
       - Seaborn integrates seamlessly with Pandas DataFrames, allowing users to directly plot data from their structured datasets with minimal code.

      Creating Complex Multi-Plot Visualizations:
       - Features like FacetGrid enable the creation of grids of plots, allowing for easy comparison of subsets of data based on categorical or temporal factors. This is particularly useful for exploring conditional relationships within a dataset.

#Practical Questions

Q.1  How do you create a 2D NumPy array and calculate the sum of each row?
"""

import numpy as np

# Create a 2D NumPy array
array_2d = np.array([[1, 2, 3],
                     [4, 5, 6],
                     [7, 8, 9]])

# Calculate the sum of each row
row_sums = np.sum(array_2d, axis=1)

# Display the array and the row sums
print("2D NumPy Array:")
print(array_2d)
print("\nSum of each row:")
print(row_sums)

"""Q.2 Write a Pandas script to find the mean of a specific column in a DataFrame."""

import pandas as pd

# Create a sample DataFrame
data = {'col1': [10, 20, 30, 40, 50],
        'col2': [100, 200, 150, 250, 300]}
df = pd.DataFrame(data)

# Specify the column for which to calculate the mean
column_name = 'col1'

# Calculate the mean of the specific column
column_mean = df[column_name].mean()

# Display the DataFrame and the calculated mean
print("DataFrame:")
display(df)
print(f"\nMean of column '{column_name}':")
print(column_mean)

"""Q.3 Create a scatter plot using Matplotlib."""

import matplotlib.pyplot as plt

# Create a scatter plot using the existing DataFrame 'df'
plt.figure(figsize=(8, 6))
plt.scatter(df['col1'], df['col2'])
plt.xlabel('col1')
plt.ylabel('col2')
plt.title('Scatter Plot of col1 vs col2')
plt.grid(True)
plt.show()

"""Q.4  How do you calculate the correlation matrix using Seaborn and visualize it with a heatmap?"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of the DataFrame')
plt.show()

"""Q.5 Generate a bar plot using Plotly."""

import plotly.express as px

# Generate a bar plot using Plotly
fig = px.bar(df, x='col1', y='col2', title='Bar Plot of col1 vs col2')
fig.show()

"""Q.6 Create a DataFrame and add a new column based on an existing column."""

import pandas as pd

# Create a sample DataFrame
data = {'colA': [1, 2, 3, 4, 5],
        'colB': [10, 20, 30, 40, 50]}
new_df = pd.DataFrame(data)

# Add a new column 'colC' based on an existing column 'colA'
new_df['colC'] = new_df['colA'] * 2

# Display the updated DataFrame
print("DataFrame with new column:")
display(new_df)

"""Q.7 Write a program to perform element-wise multiplication of two NumPy arrays."""

import numpy as np

# Create two NumPy arrays
array1 = np.array([[1, 2], [3, 4]])
array2 = np.array([[5, 6], [7, 8]])

# Perform element-wise multiplication
result_array = array1 * array2

# Display the original arrays and the result
print("Array 1:")
print(array1)
print("\nArray 2:")
print(array2)
print("\nElement-wise multiplication result:")
print(result_array)

"""Q.8 Create a line plot with multiple lines using Matplotlib."""

import matplotlib.pyplot as plt
import pandas as pd

# Create a sample DataFrame for multiple lines
data = {'x_values': [1, 2, 3, 4, 5],
        'line1': [10, 15, 13, 17, 20],
        'line2': [5, 8, 11, 14, 18]}
line_df = pd.DataFrame(data)

# Create a line plot with multiple lines
plt.figure(figsize=(10, 6))
plt.plot(line_df['x_values'], line_df['line1'], marker='o', linestyle='-', label='Line 1')
plt.plot(line_df['x_values'], line_df['line2'], marker='x', linestyle='--', label='Line 2')

plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Line Plot with Multiple Lines')
plt.legend()
plt.grid(True)
plt.show()

"""Q.9 Generate a Pandas DataFrame and filter rows where a column value is greater than a threshold."""

import pandas as pd

# Create a sample DataFrame
data = {'colA': [10, 15, 20, 25, 30],
        'colB': [100, 150, 120, 180, 200]}
filter_df = pd.DataFrame(data)

# Define a threshold value
threshold = 18

# Filter rows where 'colA' is greater than the threshold
filtered_df = filter_df[filter_df['colA'] > threshold]

# Display the original and filtered DataFrames
print("Original DataFrame:")
display(filter_df)
print(f"\nDataFrame filtered where 'colA' > {threshold}:")
display(filtered_df)

"""Q.10 Create a histogram using Seaborn to visualize a distribution."""

import seaborn as sns
import matplotlib.pyplot as plt

# Create a histogram using Seaborn
plt.figure(figsize=(8, 6))
sns.histplot(df['col1'], kde=True) # kde=True adds a kernel density estimate line
plt.xlabel('col1')
plt.ylabel('Frequency')
plt.title('Distribution of col1')
plt.show()

"""Q.11  Perform matrix multiplication using NumPy."""

import numpy as np

# Create two matrices as NumPy arrays
matrix1 = np.array([[1, 2], [3, 4]])
matrix2 = np.array([[5, 6], [7, 8]])

# Perform matrix multiplication
# The @ operator is used for matrix multiplication in NumPy
result_matrix = matrix1 @ matrix2

# Display the original matrices and the result
print("Matrix 1:")
print(matrix1)
print("\nMatrix 2:")
print(matrix2)
print("\nMatrix multiplication result:")
print(result_matrix)

"""Q.12 Use Pandas to load a CSV file and display its first 5 rows."""

import pandas as pd
import numpy as np

# Create a dummy DataFrame
dummy_data = {'colA': np.random.rand(10),
              'colB': np.random.randint(1, 100, 10),
              'colC': pd.date_range('2023-01-01', periods=10)}
dummy_df = pd.DataFrame(dummy_data)

# Save the DataFrame to a dummy CSV file
dummy_csv_path = 'dummy_data.csv'
dummy_df.to_csv(dummy_csv_path, index=False)

print(f"Created dummy CSV file at: {dummy_csv_path}")

import pandas as pd

# Specify the path to the CSV file
csv_file_path = 'dummy_data.csv' # Replace with the actual path to your CSV file

# Load the CSV file into a Pandas DataFrame
try:
    df_from_csv = pd.read_csv(csv_file_path)

    # Display the first 5 rows of the DataFrame
    print(f"First 5 rows of the DataFrame loaded from {csv_file_path}:")
    display(df_from_csv.head())

except FileNotFoundError:
    print(f"Error: The file '{csv_file_path}' was not found.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Q.13 Create a 3D scatter plot using Plotly.

"""

import plotly.express as px
import pandas as pd
import numpy as np

# Create a sample DataFrame for a 3D scatter plot
data_3d = {'colX': np.random.rand(50),
           'colY': np.random.rand(50),
           'colZ': np.random.rand(50),
           'color': np.random.randint(0, 2, 50)} # Adding a color dimension for better visualization
df_3d = pd.DataFrame(data_3d)

# Create a 3D scatter plot using Plotly
fig_3d = px.scatter_3d(df_3d, x='colX', y='colY', z='colZ',
                       color='color', title='3D Scatter Plot')
fig_3d.show()

"""#END"""